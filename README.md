# ğŸ” AI-embed-search â€” Lightweight AI Semantic Search Engine

> **Smart. Simple. Local.**  
> AI-powered semantic search in TypeScript using transformer embeddings. No cloud, no API keys â€” 100% offline.

[![NPM version](https://img.shields.io/npm/v/ai-embed-search)](https://www.npmjs.com/package/ai-embed-search)
[![Downloads](https://img.shields.io/npm/dm/ai-embed-search)](https://www.npmjs.com/package/ai-embed-search)
[![Minzipped size](https://img.shields.io/bundlephobia/minzip/ai-embed-search)](https://bundlephobia.com/package/ai-embed-search)
[![Types](https://img.shields.io/npm/types/ai-embed-search)](https://www.npmjs.com/package/ai-embed-search)
[![License](https://img.shields.io/npm/l/ai-embed-search)](./LICENSE)


---

## ğŸš€ Features

- ğŸ§  AI-powered semantic understanding
- âœ¨ Super simple API: `init`, `embed`, `search`, `clear`
- âš¡ï¸ Fast cosine similarity-based retrieval
- ğŸ“¦ In-memory vector store (no DB required)
- ğŸ§© Save/load vectors to JSON file
- ğŸ” Search filters, caching, batch embed & probabilistic softmax ranking
- ğŸ§° CLI-ready architecture
- ğŸŒ Fully offline via `@xenova/transformers` (WASM/Node)

---

## ğŸ“¦ Installation

```bash
  npm install ai-embed-search
```

or

```bash
  yarn add ai-embed-search
```
Requires Node.js â‰¥ 18 or a modern browser for WASM.

## âš¡ Quick Start

```typescript
import {embed, search, createEmbedder, initEmbedder} from 'ai-embed-search';

const embedder = await createEmbedder();
await initEmbedder({ embedder });

await embed([
  { id: '1', text: 'iPhone 15 Pro Max', meta: { brand: 'Apple', type: 'phone' } },
  { id: '2', text: 'Samsung Galaxy S24 Ultra', meta: { brand: 'Samsung', type: 'phone' } },
  { id: '3', text: 'Apple MacBook Pro', meta: { brand: 'Apple', type: 'laptop' } }
]);

const results = await search('apple phone', 2).exec();
console.log(results);
```
Result:
```typescript
[
  { id: '1', text: 'iPhone 15 Pro Max', score: 0.95, meta: { brand: 'Apple', type: 'phone' } },
  { id: '3', text: 'Apple MacBook Pro', score: 0.85, meta: { brand: 'Apple', type: 'laptop' } }
]
```

### ğŸ§  1. Initialize the Embedding Model
```typescript
import { createEmbedder, initEmbedder } from 'ai-embed-search';

const embedder = await createEmbedder();
await initEmbedder({ embedder });
```
Loads the MiniLM model via @xenova/transformers. Required once at startup.

### ğŸ“¥ 2. Add Items to the Vector Store
```typescript
import { embed } from 'ai-embed-search';

await embed([
  { id: 'a1', text: 'Tesla Model S' },
  { id: 'a2', text: 'Electric Vehicle by Tesla' }
]);
```
Embeds and stores vector representations of the given items.

### ğŸ” 3. Perform Semantic Search
```typescript
import { search } from 'ai-embed-search';

const results = await search('fast electric car', 3).exec();
```
Returns:
```typescript
[
  { id: 'a1', text: 'Tesla Model S', score: 0.95 },
  { id: 'a2', text: 'Electric Vehicle by Tesla', score: 0.85 }
]
```
### ğŸ“¦ 4. Search with Metadata
You can add metadata to each item:
```typescript
const laptops = await search('computer', 5)
    .filter(r => r.meta?.type === 'laptop')
    .exec();
```

### ğŸ’¾ 5. Search with Cached Embeddings (Advanced)
You can store precomputed embeddings in your own DB or file:
```typescript
const precomputed = {
  id: 'x1',
  text: 'Apple Watch Series 9',
  vector: [0.11, 0.32, ...] // 384-dim array
};
```
Then use cosine similarity to search across them, or build your own vector store using ai-embed-search functions.

### ğŸ§¹ 6. Clear the Vector Store

```typescript
import { removeVector, clearVectors } from 'ai-embed-search';

removeVector('a1');     // Remove by ID
clearVectors();         // Clear all vectors
```

### ğŸ¤ 7. Find Similar Items
You can retrieve the most semantically similar items to an existing one in the vector store:
```typescript
import { getSimilarItems } from 'ai-embed-search';

const similar = await getSimilarItems('1', 3);
console.log(similar);
```
Result:
```typescript
[
    { id: '2', text: 'Samsung Galaxy S24 Ultra smartphone', score: 0.93 },
    { id: '3', text: 'Apple MacBook Air M3 laptop', score: 0.87 },
    { id: '5', text: 'Dell XPS 13 ultrabook', score: 0.85 }
]
```
This is useful for recommendation systems, "related items" features, or clustering.

### ğŸ”¥ 8. Probabilistic Search with Softmax Ranking

You can rank search results **probabilistically** using a temperature-scaled softmax over cosine similarity:

```ts
import { searchWithSoftmax } from 'ai-embed-search';

const results = await searchWithSoftmax('apple wearable', 5, 0.7);
console.log(results);
```
Result:
```typescript
[
    { id: '9', text: 'Apple Watch Ultra 2', probability: 0.39 },
    { id: '3', text: 'Apple Vision Pro', probability: 0.31 },
    { id: '1', text: 'iPhone 15 Pro Max', probability: 0.18 },
    ...
]
```
How It Works:
1. Cosine similarities between the query and each item are computed.
2. The scores are scaled by a temperature T and passed through the softmax function:
```math
softmax(sáµ¢) = exp(sáµ¢ / T) / âˆ‘â±¼ exp(sâ±¼ / T)
```
Where `sáµ¢` is the similarity score for item `i`, and `T` is the temperature parameter.
3. We compute the entropy H(p) of the resulting probability distribution:
```math
H(p) = -âˆ‘áµ¢ páµ¢ log(páµ¢)
```
This measures the uncertainty in the result:
- Low entropy â‡’ confident, peaked distribution
- High entropy â‡’ uncertain, flat distribution
4. We normalize entropy to get a confidence score between 0 and 1:
```math
confidence = 1 - (H(p) / log(N))
```
Where n is the number of candidates (the maximum entropy is log(n)).

### ğŸ”¥ Temperature Intuition

| Temperature | Behavior                    | Use Case                   |
| ----------- | --------------------------- | -------------------------- |
| 0.1â€“0.5     | Very sharp, top-1 dominates | Deterministic ranking      |
| 1.0         | Balanced                    | Ranked probabilities       |
| 1.5+        | Softer, more diverse        | Random sampling / fallback |

### ğŸ“Œ Use Cases
- âœ… Probabilistic ranking â€” get soft scores for relevance
- ğŸ¯ Sampling â€” return one of top-k randomly with smart weighting
- ğŸ§  Uncertainty estimation â€” use entropy/confidence to inform users
- âš¡ï¸ Hybrid search â€” combine softmax scores with metadata (e.g., tags, categories, prices)

## ğŸ“– API Reference
### `initEmbedder()`
Initializes the embedding model. Must be called once before using `embed` or `search`.

### `embed(items: { id: string, text: string }[])`
Embeds and stores the provided items in the vector store. Each item must have a unique `id` and `text`.

### `search(query: string, limit: number)`
Performs a semantic search for the given query. Returns up to `limit` results sorted by similarity score (default is 5).

### `getSimilarItems(id: string,, limit: number)`
Finds the most similar items to the one with the given `id`. Returns up to `limit` results sorted by similarity score.

### `cacheFor(limit: number)`
Caches the embeddings for the next `limit` search queries. This is useful for optimizing performance when you know you'll be searching multiple times.

### `clearStore()`
Clears all embedded data from the vector store, freeing up memory.

### `searchWithSoftmax(query: string, limit: number, temperature: number)`
Performs a probabilistic search using softmax ranking. The `temperature` parameter controls the distribution sharpness:

## ğŸ”§ Development
- Model: [MiniLM](https://huggingface.co/xenova/bert-base-uncased) via `@xenova/transformers`
- Vector type: 384-dim float32 array
- Similarity: Cosine similarity
- Storage: In-memory vector store (no database required)
- On-premises: Fully offline, no cloud dependencies

## ğŸŒ SEO Keywords
ai search, semantic search, local ai search, vector search, transformer embeddings, cosine similarity, open source search engine, text embeddings, in-memory search, local search engine, typescript search engine, fast npm search, embeddings in JS, ai search npm package

## License
MIT Â© 2025 Peter Sibirtsev

## Contributing
Contributions are welcome! Please open an issue or submit a pull request.
